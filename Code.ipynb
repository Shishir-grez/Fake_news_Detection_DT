{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from wordcloud import WordCloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"Data\\WELFake_Dataset.csv\")\n",
    "# print(df.shape)\n",
    "# df.info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data Contains Unnamed column which is basically indexing for data , so we drop it\n",
    "#new_df = df.drop([\"Unnamed: 0\",axis=1],inplace = False)\n",
    "#new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALternate method\n",
    "#new_df1 = df.drop(df.iloc[;,0:1],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Sum of null values for each column\n",
    "#new_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #data_frame1 = new_df[new_df['title'].isnull()]\n",
    "# data_frame1['text'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By doing above code we got to know that no row with title null values and text null values overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Replace null values in \"title\" and \"text\" with \" \"\n",
    "#new_df[\"title\"] = new_df[\"title\"].fillna(\"\")\n",
    "#new_df[\"text\"] = new_df[\"text\"].fillna(\"\")\n",
    "#new_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a new column by merging title and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df[\"content\"] = new_df[\"title\"] + new_df[\"text\"]\n",
    "#new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df[\"content\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Since we have already all the text material in a column so we drop the text and title column\n",
    "#new_df.drop([\"title\",\"text\"],axis=1,inplace=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Removing URLS\n",
    "# import re\n",
    "# new_df['content'] = new_df[\"content\"].apply(lambda x: re.sub(r'http\\S+','',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Removing Punctuations\n",
    "#import string\n",
    "# translator = str.maketrans('',\"\",string.punctuation)\n",
    "# new_df[\"content\"] = new_df[\"content\"].apply(lambda x:x.translate(translator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing numerical values\n",
    "# df['Name'] = df['Name'].str.replace('\\d+', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Remove Emojis\n",
    "df.astype(str).apply(lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove html tags\n",
    "#df['overview_copy'] = df['overview_copy'].str.replace( r'<[^<>]*>', '', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #Preprocess the \"content\" column\n",
    "# stop_words = set(stopwords.words(\"english\"))\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     #Convert to lowercase\n",
    "#     text = text.lower()\n",
    "    \n",
    "#     #Remove punctuations\n",
    "#     text = text.tranlate(str.maketrans('','',string.punctuation))\n",
    "    \n",
    "#     #Tokenize the text\n",
    "#     tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "#     #Remove stop word and lemmatize the tokens\n",
    "#     tokens = [lemmatizer.lemmatize(token)for token in tokens if token not in stop_words ]\n",
    "    \n",
    "#     #Join the tokens back to form text\n",
    "#     text = ' '.join(tokens)\n",
    "    \n",
    "#     return text\n",
    "\n",
    "# new_df['content'] = new_df['content'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df.to_csv(\"lemmatized_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When we do this the unnamed column reappers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "# new_df[[\"content\",\"label\"]].to_csv(\"lemmatized_data.csv\",index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Lemmtized_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the frequency of true and false news\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (12,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot a graph\n",
    "label_counts = df['label'].value_counts()\n",
    "sns.set_style('whitegrid')\n",
    "colors = sns.color_palette('pastel')\n",
    "\n",
    "#Create a bar to visualize distribution\n",
    "plt.bar(label_counts.index,label_counts.values,width = 0.3,color = colors)\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel(\"Number of samples\")\n",
    "plt.title(\"Distribution of Labels\")\n",
    "plt.xticks([0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Float values found error \n",
    "df['context'] = df['content'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "# remove urls\n",
    "df['content'] = df['content'].replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)\n",
    "# Remove punctuations\n",
    "df[\"content\"] = df['content'].str.replace('[^\\w\\s]', '')\n",
    "df['content'] = df['content'].str.replace('\\d+', '')\n",
    "df.astype(str).apply(lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from wordcloud import WordCloud\n",
    "\n",
    "# #Convert 'content' column to string type\n",
    "# df['content'] = df['content'].astype(str)\n",
    "\n",
    "# #combine all the texts into one string\n",
    "# text = \" \".join(df['content'])\n",
    "\n",
    "# #Generate the word cloud\n",
    "# wordcloud = WordCloud( width=800,height=400 ,background_color='white',max_words=15156633 ).generate(text)\n",
    "\n",
    "# #plot the wordcloud\n",
    "# plt.figure(figsize=(20,10),facecolor=None)\n",
    "# plt.imshow(wordcloud,interpolation='bilinear')\n",
    "# plt.axis(\"off\")\n",
    "# plt.tight_layout(pad = 0)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unfortunately data not cleaned yet\n",
    "#df = df.drop([\"context\"], axis=1, inplace=False)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"Unnamed: 0\",\"context\"],axis=1,inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"content\",\"label\"]].to_csv(\"cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To display the wordcloud\n",
    "from PIL import Image\n",
    "img = Image.open('Word_Cloud.png')\n",
    "plt.imshow(img)\n",
    "# turn off grid\n",
    "plt.grid(False)\n",
    "\n",
    "# disable graph axes lining\n",
    "plt.axis('off')\n",
    "\n",
    "# remove x and y ticks\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets measure the frequency of specific words in fake news and true news\n",
    "from collections import Counter\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "df['content'] = df['content'].apply(lambda x: ' '.join([word.lower() for word in x.split() if word.lower() not in stop_words]))\n",
    "\n",
    "df['content'] = df['content'].str.replace('[{}]'.format(string.punctuation), '')\n",
    "df['content'] = df['content'].str.replace('.', '')\n",
    "\n",
    "df['content'] = df['content'].fillna('')\n",
    "# Count word frequencies\n",
    "fake_words = ' '.join(df[df['label'] == 0]['content']).translate(\n",
    "    str.maketrans('', '', string.punctuation))\n",
    "true_words = ' '.join(df[df['label'] == 1]['content']).translate(\n",
    "    str.maketrans('', '', string.punctuation))\n",
    "\n",
    "fake_counts = Counter(fake_words.split())\n",
    "true_counts = Counter(true_words.split())\n",
    "\n",
    "# Visualize the frequency of the most common words in fake news articles\n",
    "fake_wordcloud = WordCloud(background_color='white', width=800,height=400).generate_from_frequencies(fake_counts)\n",
    "plt.imshow(fake_wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Visualize the frequency of the most common words in true news articles\n",
    "true_wordcloud = WordCloud(background_color='white', width=800,height=400).generate_from_frequencies(true_counts)\n",
    "plt.imshow(true_wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.read_csv(\"cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Float values found error\n",
    "news_df['context'] = news_df['content'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of news lengths\n",
    "import numpy as np\n",
    "\n",
    "# Check for NaN or float values in the 'content' column and replace them with empty strings\n",
    "news_df['content'] = np.where(news_df['content'].isnull() | news_df['content'].astype(str).str.isdigit(),'', news_df['content'])\n",
    "\n",
    "# Calculate the length of each news content\n",
    "news_df['length'] = news_df['content'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Plot the distribution of news lengths\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "sns.histplot(x='length', data=news_df)\n",
    "plt.title('Distribution of News Lengths')\n",
    "plt.xlabel('Length')\n",
    "plt.xlim(0, 1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
